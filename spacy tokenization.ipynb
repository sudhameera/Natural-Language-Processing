{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f93e9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.2-cp37-cp37m-win_amd64.whl (12.4 MB)\n",
      "     ---------------------------------------- 12.4/12.4 MB 7.4 MB/s eta 0:00:00\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp37-cp37m-win_amd64.whl (39 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "     -------------------------------------- 50.1/50.1 kB 848.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\venka\\anaconda3\\envs\\aiml\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\venka\\anaconda3\\envs\\aiml\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "     -------------------------------------- 57.0/57.0 kB 600.1 kB/s eta 0:00:00\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "     -------------------------------------- 45.9/45.9 kB 757.9 kB/s eta 0:00:00\n",
      "Collecting typing-extensions<4.5.0,>=3.7.4.1\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\venka\\anaconda3\\envs\\aiml\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp37-cp37m-win_amd64.whl (122 kB)\n",
      "     -------------------------------------- 122.6/122.6 kB 1.8 MB/s eta 0:00:00\n",
      "Collecting thinc<8.3.0,>=8.1.8\n",
      "  Downloading thinc-8.2.1-cp37-cp37m-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 10.6 MB/s eta 0:00:00\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp37-cp37m-win_amd64.whl (25 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.6/181.6 kB 2.8 MB/s eta 0:00:00\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\venka\\anaconda3\\envs\\aiml\\lib\\site-packages (from spacy) (22.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\venka\\anaconda3\\envs\\aiml\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\venka\\anaconda3\\envs\\aiml\\lib\\site-packages (from spacy) (4.66.1)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp37-cp37m-win_amd64.whl (482 kB)\n",
      "     -------------------------------------- 482.7/482.7 kB 7.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\venka\\anaconda3\\envs\\aiml\\lib\\site-packages (from spacy) (1.21.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\venka\\anaconda3\\envs\\aiml\\lib\\site-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.11.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.3 in c:\\users\\venka\\anaconda3\\envs\\aiml\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\venka\\anaconda3\\envs\\aiml\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\venka\\anaconda3\\envs\\aiml\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.5.0-py3-none-any.whl (407 kB)\n",
      "     -------------------------------------- 407.5/407.5 kB 5.1 MB/s eta 0:00:00\n",
      "  Downloading pydantic-2.4.2-py3-none-any.whl (395 kB)\n",
      "     -------------------------------------- 395.8/395.8 kB 6.2 MB/s eta 0:00:00\n",
      "  Downloading pydantic-2.4.1-py3-none-any.whl (395 kB)\n",
      "     -------------------------------------- 395.3/395.3 kB 6.2 MB/s eta 0:00:00\n",
      "  Downloading pydantic-2.4.0-py3-none-any.whl (395 kB)\n",
      "     -------------------------------------- 395.4/395.4 kB 4.9 MB/s eta 0:00:00\n",
      "  Downloading pydantic-2.3.0-py3-none-any.whl (374 kB)\n",
      "     -------------------------------------- 374.5/374.5 kB 1.7 MB/s eta 0:00:00\n",
      "  Downloading pydantic-2.2.1-py3-none-any.whl (373 kB)\n",
      "     -------------------------------------- 373.4/373.4 kB 5.8 MB/s eta 0:00:00\n",
      "  Downloading pydantic-2.2.0-py3-none-any.whl (373 kB)\n",
      "     -------------------------------------- 373.2/373.2 kB 4.7 MB/s eta 0:00:00\n",
      "  Downloading pydantic-2.1.1-py3-none-any.whl (370 kB)\n",
      "     -------------------------------------- 370.9/370.9 kB 4.6 MB/s eta 0:00:00\n",
      "  Downloading pydantic-2.1.0-py3-none-any.whl (370 kB)\n",
      "     -------------------------------------- 370.8/370.8 kB 5.7 MB/s eta 0:00:00\n",
      "  Downloading pydantic-2.0.3-py3-none-any.whl (364 kB)\n",
      "     -------------------------------------- 364.0/364.0 kB 1.9 MB/s eta 0:00:00\n",
      "  Downloading pydantic-2.0.2-py3-none-any.whl (359 kB)\n",
      "     -------------------------------------- 359.1/359.1 kB 1.6 MB/s eta 0:00:00\n",
      "  Downloading pydantic-2.0.1-py3-none-any.whl (358 kB)\n",
      "     -------------------------------------- 358.4/358.4 kB 4.5 MB/s eta 0:00:00\n",
      "  Downloading pydantic-2.0-py3-none-any.whl (355 kB)\n",
      "     -------------------------------------- 355.6/355.6 kB 5.6 MB/s eta 0:00:00\n",
      "  Downloading pydantic-1.10.13-cp37-cp37m-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 7.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\venka\\anaconda3\\envs\\aiml\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\venka\\anaconda3\\envs\\aiml\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\venka\\anaconda3\\envs\\aiml\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\venka\\anaconda3\\envs\\aiml\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.3-py3-none-any.whl (34 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp37-cp37m-win_amd64.whl (6.6 MB)\n",
      "     ---------------------------------------- 6.6/6.6 MB 8.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\venka\\anaconda3\\envs\\aiml\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\venka\\anaconda3\\envs\\aiml\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "     -------------------------------------- 45.0/45.0 kB 740.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\venka\\anaconda3\\envs\\aiml\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: cymem, typing-extensions, spacy-loggers, spacy-legacy, smart-open, murmurhash, langcodes, blis, wasabi, pydantic, preshed, catalogue, srsly, cloudpathlib, typer, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.5.1\n",
      "    Uninstalling pydantic-2.5.1:\n",
      "      Successfully uninstalled pydantic-2.5.1\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.3 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-1.10.13 smart-open-6.4.0 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.1 typer-0.9.0 typing-extensions-4.4.0 wasabi-1.1.2 weasel-0.3.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.23.1 requires protobuf<5,>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
      "pydantic-core 2.14.3 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.4.0 which is incompatible.\n",
      "openai 1.3.4 requires typing-extensions<5,>=4.5, but you have typing-extensions 4.4.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "449f088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94fe796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.strange\n",
      "likes\n",
      "pav\n",
      "bhaji\n",
      "of\n",
      "mumbai\n",
      "as\n",
      "it\n",
      "costs\n",
      "only\n",
      "2\n",
      "$\n",
      "per\n",
      "plate\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Dr.strange likes pav bhaji of mumbai as it costs only 2$ per plate.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef7aa2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange\n",
      "likes\n",
      "pav\n",
      "bhaji\n",
      "of\n",
      "mumbai\n",
      "as\n",
      "it\n",
      "costs\n",
      "only\n",
      "2\n",
      "$\n",
      "per\n",
      "plate\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr.Strange likes pav bhaji of mumbai as it costs only 2$ per plate.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbe3d635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dr."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8b8ae73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c3ffbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Strange likes pav bhaji of"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da2c3264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '_bulk_merge',\n",
       " '_context',\n",
       " '_get_array_attrs',\n",
       " '_realloc',\n",
       " '_vector',\n",
       " '_vector_norm',\n",
       " 'cats',\n",
       " 'char_span',\n",
       " 'copy',\n",
       " 'count_by',\n",
       " 'doc',\n",
       " 'ents',\n",
       " 'extend_tensor',\n",
       " 'from_array',\n",
       " 'from_bytes',\n",
       " 'from_dict',\n",
       " 'from_disk',\n",
       " 'from_docs',\n",
       " 'from_json',\n",
       " 'get_extension',\n",
       " 'get_lca_matrix',\n",
       " 'has_annotation',\n",
       " 'has_extension',\n",
       " 'has_unknown_spaces',\n",
       " 'has_vector',\n",
       " 'is_nered',\n",
       " 'is_parsed',\n",
       " 'is_sentenced',\n",
       " 'is_tagged',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'mem',\n",
       " 'noun_chunks',\n",
       " 'noun_chunks_iterator',\n",
       " 'remove_extension',\n",
       " 'retokenize',\n",
       " 'sentiment',\n",
       " 'sents',\n",
       " 'set_ents',\n",
       " 'set_extension',\n",
       " 'similarity',\n",
       " 'spans',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'to_array',\n",
       " 'to_bytes',\n",
       " 'to_dict',\n",
       " 'to_disk',\n",
       " 'to_json',\n",
       " 'to_utf8_array',\n",
       " 'user_data',\n",
       " 'user_hooks',\n",
       " 'user_span_hooks',\n",
       " 'user_token_hooks',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f91ccf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"Let's go to N.Y!\")\n",
    "\n",
    "for word in doc:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2cb44d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y\n",
      "!\n",
      ".\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "doc=nlp('''\"Let's go to N.Y!.\"''')\n",
    "\n",
    "for word in doc:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16067a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony\n",
      "gave\n",
      "two\n",
      "$\n",
      "to\n",
      "peter\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tony gave two $ to peter \")\n",
    "\n",
    "for word in doc:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "860aaabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tony"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word0=doc[0]\n",
    "word0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3badd211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf40f8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word0.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02660518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word0.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bae7f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1152b80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "befb48f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d28d8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55d63fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "two"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de9ee753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2].like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80a38623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony ==> index: 0 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "gave ==> index: 1 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "two ==> index: 2 is_alpha: True is_punct: False is_currency: False like_num: True\n",
      "$ ==> index: 3 is_alpha: False is_punct: False is_currency: True like_num: False\n",
      "to ==> index: 4 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "peter ==> index: 5 is_alpha: True is_punct: False is_currency: False like_num: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print (token, \"==>\",\"index:\",token.i,\n",
    "          \"is_alpha:\",token.is_alpha,\n",
    "          \"is_punct:\",token.is_punct,\n",
    "          \"is_currency:\",token.is_currency,\n",
    "          \"like_num:\",token.like_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d26c23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a409451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a13386da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75fb2d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '\\n',\n",
       " 'Dayton high school, 8 Grade students information\\n',\n",
       " '=====================================\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Name         birth day         email\\n',\n",
       " '---------         -------------         ---------\\n',\n",
       " '\\n',\n",
       " 'Virat          5 june,1882      virat@kohli.com\\n',\n",
       " 'Maria        12 April,1856   maria@shapova.com\\n',\n",
       " 'serena       16may,1646        serena@williams.com\\n',\n",
       " 'joe               5 nov, 1963           joe@root.com']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"students.txt\") as f:\n",
    "    text = f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2ece54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n \\n Dayton high school, 8 Grade students information\\n =====================================\\n \\n \\n Name         birth day         email\\n ---------         -------------         ---------\\n \\n Virat          5 june,1882      virat@kohli.com\\n Maria        12 April,1856   maria@shapova.com\\n serena       16may,1646        serena@williams.com\\n joe               5 nov, 1963           joe@root.com'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a9d29d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n \\n Dayton high school, 8 Grade students information\\n =====================================\\n \\n \\n Name         birth day         email\\n ---------         -------------         ---------\\n \\n Virat          5 june,1882      virat@kohli.com\\n Maria        12 April,1856   maria@shapova.com\\n serena       16may,1646        serena@williams.com\\n joe               5 nov, 1963           joe@root.com'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45f43e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virat@kohli.com\n",
      "maria@shapova.com\n",
      "serena@williams.com\n",
      "joe@root.com\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53935556",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails=[]\n",
    "for word in doc:\n",
    "    if word.like_email:\n",
    "        emails.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e95334ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[virat@kohli.com, maria@shapova.com, serena@williams.com, joe@root.com]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0491fce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virat@kohli.com', 'maria@shapova.com', 'serena@williams.com', 'joe@root.com']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails=[]\n",
    "for word in doc:\n",
    "    if word.like_email:\n",
    "        emails.append(word.text)\n",
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8b1e337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[gimme, double, cheese, extra, large, healthy, pizza]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list comprehension\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "\n",
    "tokens =[token for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d72be520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[gim, me, double, cheese, extra, large, healthy, pizza]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding special rule in tokenizer\n",
    "#customize tokenizer\n",
    "\n",
    "from spacy.symbols import ORTH\n",
    "nlp.tokenizer.add_special_case(\"gimme\",[\n",
    "    {ORTH:\"gim\"},\n",
    "    {ORTH:\"me\"}\n",
    "    \n",
    "])\n",
    "\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "\n",
    "tokens = [token for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b7ff90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.data.gov/\n",
      "http://www.science\n",
      "http://data.gov.uk/.\n",
      "http://www3.norc.org/gss+website/\n",
      "http://www.europeansocialsurvey.org/.\n"
     ]
    }
   ],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "\n",
    "word = nlp(text)\n",
    "for token in word:\n",
    "    if token.like_url:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a327871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[http://www.data.gov/,\n",
       " http://www.science,\n",
       " http://data.gov.uk/.,\n",
       " http://www3.norc.org/gss+website/,\n",
       " http://www.europeansocialsurvey.org/.]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails=[]\n",
    "word = nlp(text)\n",
    "for token in word:\n",
    "    if token.like_url:\n",
    "        emails.append(token)\n",
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "698abbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[two, 500]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "doc = nlp(transactions)\n",
    "transaction=[]\n",
    "for token in doc:\n",
    "    if token.like_num and doc[token.i+1].is_currency:\n",
    "        transaction.append(token)\n",
    "transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0046ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "263580\n",
    "523968743\n",
    "69857410\n",
    "5623987325\n",
    "5876523796358278\n",
    "859621395\n",
    "32986325269\n",
    "45892388965789\n",
    "325699874256\n",
    "1426987456\n",
    "325698745\n",
    "65296013573\n",
    "5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6a1a004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(transactions)\n",
    "\n",
    "for token in doc:\n",
    "    if token.like_num and doc[token.i+1].is_currency:\n",
    "        print(token,doc[token.i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c46e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
